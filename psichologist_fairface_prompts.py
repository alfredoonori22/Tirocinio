import os
import torch
import argparse
from tqdm import tqdm
from CoOp.clip import clip
from global_variables import *


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--fpath", type=str, default="/homes/aonori/Tirocinio/CoOp/output/fairface/CoOp/gender/vit_b32_16shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-200", help="Path to the learned prompt")
    parser.add_argument("--model", type=str, default="ViT-B/32", help="Baseline model for CLIP")
    args = parser.parse_args()

    assert os.path.exists(args.fpath)

    # Retrieve context generated by the model in fpath
    prompt_learner = torch.load(args.fpath, map_location="cuda")["state_dict"]
    ctx = prompt_learner["ctx"]
    ctx = ctx.float()
    print(f"Size of context: {ctx.shape}")

    # Label for testing the model
    labels = ['Competent', 'Intelligent', 'Skillful', 'Honest', 'Trustworthy', 'Empathetic', 'Motivated', 'Patient']
    tokenized_labels = [clip.tokenize(label) for label in labels]

    # Find the most similar token from the embeddings
    token_embedding = model.token_embedding.weight
    distance = torch.cdist(ctx, token_embedding)
    print(f"Size of distance matrix: {distance.shape}")
    tokenized_coop = torch.argsort(distance, dim=1)[:, 0]

    with torch.no_grad():
        # Build the prompt
        encoded_prompts = create_prompt(tokenized_labels, tokenized_coop)
        # Get faces from images in dairface datast, with their label [race-gender]
        faces = [Face(face) for face in tqdm(fairface)]
        # Run clip with the faces and the different prompt (find the nearest one to the proposed image)
        fairface_labels, predictions = classify(faces, encoded_prompts, labels)

    # Create the heatmap to visualize the data
    percentage_matrix = create_Heatmap(fairface_labels, predictions)
