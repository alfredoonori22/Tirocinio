import os
import argparse
import torch
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import datasets

from CoOp.clip.simple_tokenizer import SimpleTokenizer
from CoOp.clip import clip
from PIL import Image
from collections import Counter


class Face:
  def __init__(self, fairface_face):
    self.race = fairface.features['race'].int2str(fairface_face['race'])
    self.gender = fairface.features['gender'].int2str(fairface_face['gender'])
    self.label = f'{self.race}_{self.gender}'

    with torch.no_grad():
      image_input = preprocess(fairface_face['image']).unsqueeze(0).to(device)
      self.image_features = model.encode_image(image_input)
      self.image_features /= self.image_features.norm(dim=-1, keepdim=True)


def classify(faces, encoded_prompts, class_labels):
  labels, predictions = [], []

  for face in faces:
    # distribuzione di probabilità che misura la similarità tra le caratteristiche dell'immagine e i prompt di testo
    similarity = (100.0 * face.image_features @ encoded_prompts.T).softmax(dim=-1)

    # restituirà il valore massimo (value) e l'indice corrispondente (index)
    [value], [index] = similarity[0].topk(1)

    #  conterrà l'etichetta di classe prevista per l'immagine in base al confronto con i prompt di testo
    prediction = class_labels[index]

    labels.append(face.label)
    predictions.append(prediction)

  return labels, predictions


def create_Heatmap(fairface_labels, predictions):
  pairs = list(zip(fairface_labels, predictions))
  counts = Counter(pairs)

  unique_labels = sorted(set(fairface_labels))
  unique_predictions = sorted(set(predictions))
  matrix = np.zeros((len(unique_labels), len(unique_predictions)))

  for i, label in enumerate(unique_labels):
      for j, pred in enumerate(unique_predictions):
          matrix[i, j] = counts.get((label, pred), 0)

  row_sums = matrix.sum(axis=1, keepdims=True)
  percentage_matrix = (matrix / row_sums) * 100

  plt.figure(figsize=(10, 8))
  sns.set(font_scale=0.7)
  ax = sns.heatmap(percentage_matrix, annot=True, fmt='.2f', cmap='Greens',
                  xticklabels=unique_predictions,
                  yticklabels=unique_labels,
                  annot_kws={"size": 8})
  plt.xlabel('Predicted')
  plt.ylabel('True')
  plt.title('Prediction Distribution Percentage')
  plt.savefig('heatmap.jpg')

  return percentage_matrix


if __name__ == "__main__":

    parser = argparse.ArgumentParser()
    parser.add_argument("--fpath", type=str, default="/homes/aonori/Tirocinio/CoOp/output/fairface/CoOp/gender/vit_b32_16shots/nctx16_cscFalse_ctpend/seed3/prompt_learner/model.pth.tar-200", help="Path to the learned prompt")
    parser.add_argument("--model", type=str, default="ViT-B/32", help="Baseline model for CLIP")
    args = parser.parse_args()

    assert os.path.exists(args.fpath)

    # Retrieve model and dataset
    device = 'cuda' if torch.cuda.is_available() else 'cpu'
    model, preprocess = clip.load(name=args.model, device=device)
    fairface = datasets.load_dataset('HuggingFaceM4/FairFace')['validation']

    # Retrieve context generated by the model in fpath
    prompt_learner = torch.load(args.fpath, map_location="cuda")["state_dict"]
    ctx = prompt_learner["ctx"]
    ctx = ctx.float()
    print(f"Size of context: {ctx.shape}")

    # Label for testing the model
    labels = ['Competent', 'Intelligent', 'Skillful', 'Honest', 'Trustworthy', 'Empathetic', 'Motivated', 'Patient']

    with torch.no_grad():
        # Tokenize and encode the labels
        tokenized_labels = torch.cat([clip.tokenize(label) for label in labels]).to(device)

        encoded_labels = model.encode_text(tokenized_labels)
        encoded_labels /= encoded_labels.norm(dim=-1, keepdim=True)
        # encoded_labels_pairs = encoded_labels.view(encoded_labels.size(0), 2, 512)

        # Build the different prompts (one for each label), concatenating the context learned by CoOp and the different encoded labels
        encoded_prompts = [torch.cat([ctx, label.unsqueeze(0)]) for label in encoded_labels]

        # Get faces from images in dairface datast, with their label [race-gender]
        faces = [Face(face) for face in fairface]
        # Run clip with the faces and the different prompt (find the nearest one to the proposed image)
        fairface_labels, predictions = classify(faces, encoded_prompts, labels)

    # Create the heatmap to visualize the data
    percentage_matrix = create_Heatmap(fairface_labels, predictions)
